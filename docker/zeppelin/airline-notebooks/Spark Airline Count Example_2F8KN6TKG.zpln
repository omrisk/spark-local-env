{
  "paragraphs": [
    {
      "title": "Use Generic Inline Configuration instead of Interpreter Setting",
      "text": "%md\n## Setting up the spark environment\nThis is the starting point of the wordcount example.\nWe will setup the basic configurations needed to setup our spark application and read information from our mock s3 bucket.\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 07:50:17.815",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSetting up the spark environment\u003c/h2\u003e\n\u003cp\u003eThis is the starting point of the airline count example.\u003cbr /\u003e\nWe will setup the basic configurations needed to setup our spark application and read information from our mock s3 bucket.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588214762316_737450410",
      "id": "20180531-100923_1307061430",
      "dateCreated": "2020-04-30 10:46:02.000",
      "dateStarted": "2021-02-15 15:12:17.416",
      "dateFinished": "2021-02-15 15:12:17.437",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## A little bit about Zeppelin notebooks\n\nBefore we start let\u0027s explain a little about Zeppelin notebooks,a Zeppelin notebook allows users to write paragraphs (like this one) and run them in the browser.\n\nThe first word preceded by `%` tells Zeppelin what interpreter or \"engine\" should run the paragraph.\nFor example, this paragraph is run by the `%md` interpreter, if you click the `Show Editor` button (or `control+option+E` on your keyboard) on the top-right of this paragraph you\u0027ll see the markdown that generated this explanation.\n\nTry adding some text and click the `Run this paragraph` button on the top-right of this paragraph.\n\nZeppelin will launch the markdown interpreter to re-proceess the markdown in this paragraph and update the output area.\n\nWhen you\u0027re done, hide the editor if you like.",
      "user": "anonymous",
      "dateUpdated": "2021-02-24 06:15:43.442",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eA little bit about Zeppelin notebooks\u003c/h2\u003e\n\u003cp\u003eBefore we start let\u0026rsquo;s explain a little about Zeppelin notebooks,a Zeppelin notebook allows users to write paragraphs (like this one) and run them in the browser.\u003c/p\u003e\n\u003cp\u003eThe first word preceded by \u003ccode\u003e%\u003c/code\u003e tells Zeppelin what interpreter or \u0026ldquo;engine\u0026rdquo; should run the paragraph.\u003cbr /\u003e\nFor example, this paragraph is run by the \u003ccode\u003e%md\u003c/code\u003e interpreter, if you click the \u003ccode\u003eShow Editor\u003c/code\u003e button (or \u003ccode\u003econtrol+option+E\u003c/code\u003e on your keyboard) on the top-right of this paragraph you\u0026rsquo;ll see the markdown that generated this explanation.\u003c/p\u003e\n\u003cp\u003eTry adding some text and click the \u003ccode\u003eRun this paragraph\u003c/code\u003e button on the top-right of this paragraph.\u003c/p\u003e\n\u003cp\u003eZeppelin will launch the markdown interpreter to re-proceess the markdown in this paragraph and update the output area.\u003c/p\u003e\n\u003cp\u003eWhen you\u0026rsquo;re done, hide the editor if you like.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614146895981_437874226",
      "id": "paragraph_1614146895981_437874226",
      "dateCreated": "2021-02-24 06:08:15.981",
      "dateStarted": "2021-02-24 06:15:42.094",
      "dateFinished": "2021-02-24 06:15:45.245",
      "status": "FINISHED"
    },
    {
      "title": "Inline Configuration for standalone spark cluster",
      "text": "%spark.conf\n# This will point to the path we installed spark on the zeppelin docker image\nSPARK_HOME /opt/zeppelin/spark/\n\n# Will set the application name in the spark UI\nspark.app.name Zeppelin\n\n# set driver memory to 8g\nspark.driver.memory 8g\n\n# set executor memory 4g\nspark.executor.memory  4g\nspark.executor.cores 4\n\n# set execution mode\nmaster standalone\n\n# Will set the spark driver to be our zeppelin docker\nspark.submit.deployMode\tclient\n\n# Needed for our writing efficiently to our s3 minio docker - read more at https://spark.apache.org/docs/3.0.1/cloud-integration.html\nspark.speculation false\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2\nspark.hadoop.fs.s3.impl org.apache.hadoop.fs.s3a.S3AFileSystem\n\n# Set to allow us to \"point\" to our mock s3 and use it interchangibly \nspark.hadoop.fs.s3a.endpoint http://minio:9000\nspark.hadoop.fs.s3a.path.style.access true\nspark.hadoop.fs.s3a.connection.ssl.enabled false\n\n# Any other spark properties can be set here. Here\u0027s avaliable spark configruation you can set. (http://spark.apache.org/docs/latest/configuration.html)\n\n# When you\u0027re done reading, run this paragraph to set apply the settings",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 07:57:42.732",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588214762316_1311021507",
      "id": "20180531-101615_648039641",
      "dateCreated": "2020-04-30 10:46:02.000",
      "dateStarted": "2021-03-03 07:57:42.761",
      "dateFinished": "2021-03-03 07:57:42.871",
      "status": "FINISHED"
    },
    {
      "text": "%md \n### Sanity test!\nYou can run the paragraph on the right to try a simple spark function that reads a local parquet file and \nprints it to the output section.\nThis will launch a spark application that will read a small `parquet` file and print it\u0027s schema and contents \nto the notebook.\n\nThis creates a Dataframe, which is a representation of our data organised by columns.\n\nYou can visit the [spark UI](localhost:8080) or the [spark driver](localhost:4040) to check it\u0027s progress.",
      "user": "anonymous",
      "dateUpdated": "2021-03-02 15:52:40.371",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "tableHide": false,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorHide": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eSanity test!\u003c/h3\u003e\n\u003cp\u003eYou can run the paragraph on the right to try a simple spark function that reads a local parquet file and\u003cbr /\u003e\nprints it to the output section.\u003cbr /\u003e\nThis will launch a spark application that will read a small \u003ccode\u003eparquet\u003c/code\u003e file and print it\u0026rsquo;s schema and contents\u003cbr /\u003e\nto the notebook.\u003c/p\u003e\n\u003cp\u003eThis creates a Dataframe, which is a representation of our data organised by columns.\u003c/p\u003e\n\u003cp\u003eYou can visit the \u003ca href\u003d\"localhost:8080\"\u003espark UI\u003c/a\u003e or the \u003ca href\u003d\"localhost:4040\"\u003espark driver\u003c/a\u003e to check it\u0026rsquo;s progress.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613400399813_1327092738",
      "id": "paragraph_1613400399813_1327092738",
      "dateCreated": "2021-02-15 14:46:39.813",
      "dateStarted": "2021-03-02 15:52:40.372",
      "dateFinished": "2021-03-02 15:52:40.465",
      "status": "FINISHED"
    },
    {
      "title": "",
      "text": "%spark\n\nimport spark.implicits._\n\nval df \u003d spark.read.format(\"parquet\").load(\"/zeppelin/example-files/users.parquet\")  \ndf.printSchema\ndf.show\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 08:05:25.746",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588214762324_60233930",
      "id": "20180530-222838_1995256600",
      "dateCreated": "2020-04-30 10:46:02.000",
      "dateStarted": "2021-03-03 08:05:25.782",
      "dateFinished": "2021-03-03 08:06:10.263",
      "status": "FINISHED"
    },
    {
      "text": "%md \n## Reading from s3\nSince we\u0027ve setup a [minio](https://github.com/minio/minio) s3 docker, we can now read \nour airlines dataset into our spark application",
      "user": "anonymous",
      "dateUpdated": "2021-03-02 15:50:12.035",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "editorHide": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 90.4545,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eReading from s3\u003c/h2\u003e\n\u003cp\u003eSince we\u0026rsquo;ve setup a \u003ca href\u003d\"https://github.com/minio/minio\"\u003eminio\u003c/a\u003e s3 docker, we can now read\u003cbr /\u003e\nour airlines dataset into our spark application\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613402011974_1184306943",
      "id": "paragraph_1613402011974_1184306943",
      "dateCreated": "2021-02-15 15:13:31.974",
      "dateStarted": "2021-02-24 06:14:38.526",
      "dateFinished": "2021-02-24 06:14:38.542",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\nimport spark.implicits._\n\nval airlines \u003d spark.read.format(\"csv\").load(\"s3a://word-count/flights-data/airlines.csv\")\n\nairlines.printSchema\nairlines.show",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 08:14:37.602",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorHide": false,
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613403849055_1657905061",
      "id": "paragraph_1613403849055_1657905061",
      "dateCreated": "2021-02-15 15:44:09.056",
      "dateStarted": "2021-03-03 08:14:37.633",
      "dateFinished": "2021-03-03 08:14:44.866",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## What\u0027s in a schema?\nIn the last paragraph we loaded our data set from MinIO and printed the schema.\nThe schema is a the description of the structure of your data, it defines how Spark treats each column of the CSV.\nSo it\u0027s what defines if we treat a column containing a `123` as a string, integer or double.\n\nReading data this way, creates a dataframe, this is a \"row\" of data that has columns according to the schema we defined.\n\nLet\u0027s define a schema in the next `%spark` paragraph and read our flight data.\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 08:22:19.408",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eWhat\u0026rsquo;s in a schema?\u003c/h2\u003e\n\u003cp\u003eIn the last paragraph we loaded our data set from MinIO and printed the schema.\u003cbr /\u003e\nThe schema is a the description of the structure of your data, it defines how Spark treats each column of the CSV.\u003cbr /\u003e\nSo it\u0026rsquo;s what defines if we treat a column containing a \u003ccode\u003e123\u003c/code\u003e as a string, integer or double.\u003c/p\u003e\n\u003cp\u003eReading data this way, creates a dataframe, this is a \u0026ldquo;row\u0026rdquo; of data that has columns according to the schema we defined.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s define a schema in the next \u003ccode\u003e%spark\u003c/code\u003e paragraph and read our flight data.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614697262960_1411699733",
      "id": "paragraph_1614697262960_1411699733",
      "dateCreated": "2021-03-02 15:01:02.960",
      "dateStarted": "2021-03-03 08:22:19.410",
      "dateFinished": "2021-03-03 08:22:22.484",
      "status": "FINISHED"
    },
    {
      "text": "%spark\nimport spark.implicits._\nimport org.apache.spark.sql.types._\n\nval airlineSchema \u003d StructType(Array(\n    StructField(\"id\",IntegerType,true),\n    StructField(\"airlineName\",StringType,true),\n    StructField(\"alias\",StringType,true),\n    StructField(\"iataCode\", StringType, true),\n    StructField(\"icaoCode\", StringType, true),\n    StructField(\"callsign\", StringType, true),\n    StructField(\"country\", StringType, true),\n    StructField(\"active\", StringType, true)\n  ))\n  \n  val airlinesWithSchema \u003d  spark.read.format(\"csv\")\n                                .schema(airlineSchema)\n                                .load(\"s3a://word-count/flights-data/airlines.csv\")\n\nairlinesWithSchema.printSchema\nairlinesWithSchema.show",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 08:22:46.633",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614698363348_2020104831",
      "id": "paragraph_1614698363348_2020104831",
      "dateCreated": "2021-03-02 15:19:23.349",
      "dateStarted": "2021-03-03 08:22:46.648",
      "dateFinished": "2021-03-03 08:22:49.047",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Dataframe vs DataSet\nIn the previous paragraph we saw how we can define a schema describing a row from the CSV.\n\nAn additional way to handle this would be to use a DataSet.\n\nA DataSet allow us to read data into a Dataframe but to use a strongly-typed API.\n\nSo in our case we could declare an `Airline` class and load our data into a typed Scala object.\n\nLet\u0027s see an example in the next paragraph.\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-02 19:21:50.570",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDataframe vs DataSet\u003c/h2\u003e\n\u003cp\u003eIn the previous paragraph we saw how we can define a schema describing a row from the CSV.\u003c/p\u003e\n\u003cp\u003eAn additional way to handle this would be to use a DataSet.\u003c/p\u003e\n\u003cp\u003eA DataSet allow us to read data into a Dataframe but to use a strongly-typed API.\u003c/p\u003e\n\u003cp\u003eSo in our case we could declare an \u003ccode\u003eAirline\u003c/code\u003e class and load our data into a typed Scala object.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s see an example in the next paragraph.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614712387868_2078024181",
      "id": "paragraph_1614712387868_2078024181",
      "dateCreated": "2021-03-02 19:13:07.882",
      "dateStarted": "2021-03-02 19:21:49.308",
      "dateFinished": "2021-03-02 19:21:49.332",
      "status": "FINISHED"
    },
    {
      "text": "%spark\nimport spark.implicits._\n\ncase class AirlineDS(id: Integer, airlineName: String, alias: String,\n                    iataCode: String, icaoCode: String, callSign: String, country: String, active: String )\n\n\nval airlineDS \u003d spark.read.format(\"csv\")\n                                .schema(airlineSchema)\n                                .load(\"s3a://word-count/flights-data/airlines.csv\").as[AirlineDS]\n                                \nairlineDS.printSchema\nairlineDS.show\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 08:42:12.424",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614712913459_10545824",
      "id": "paragraph_1614712913459_10545824",
      "dateCreated": "2021-03-02 19:21:53.459",
      "dateStarted": "2021-03-03 08:42:12.473",
      "dateFinished": "2021-03-03 08:42:15.424",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n## DataSets as Objects\n\nIn the previous paragraph we saw how we can read data and then represent it as a scala case class.\n\nThis can be useful to ensure that typing is checked when running tests, when working in strongly typed languages as it can allow for compile time checking and anlaysis.\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-02 19:30:37.271",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDataSets as Objects\u003c/h2\u003e\n\u003cp\u003eIn the previous paragraph we saw how we can read data and then represent it as a scala case class.\u003c/p\u003e\n\u003cp\u003eThis can be useful to ensure that typing is checked when running tests, when working in strongly typed languages as it can allow for compile time checking and anlaysis.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614713294836_1925595498",
      "id": "paragraph_1614713294836_1925595498",
      "dateCreated": "2021-03-02 19:28:14.836",
      "dateStarted": "2021-03-02 19:30:37.274",
      "dateFinished": "2021-03-02 19:30:37.325",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Count all the things \nA common problem we usually need to solve is aggregation, how many times did something appear?\n\nIn this case we may be interested to know how many airlines are based in the United States?\nHow many in Russia?\n\nSince we\u0027ve created the `airlineDS` dataset let\u0027s try to count what are the countries with the most active airlines?",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 09:03:05.537",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eCount all the things\u003c/h2\u003e\n\u003cp\u003eA common problem we usually need to solve is aggregation, how many times did something appear?\u003c/p\u003e\n\u003cp\u003eIn this case we may be interested to know how many airlines are based in the United States?\u003cbr /\u003e\nHow many in Russia?\u003c/p\u003e\n\u003cp\u003eSince we\u0026rsquo;ve created the \u003ccode\u003eairlineDS\u003c/code\u003e dataset let\u0026rsquo;s try to count what are the countries with the most active airlines?\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613718993550_1857939506",
      "id": "paragraph_1613718993550_1857939506",
      "dateCreated": "2021-02-19 07:16:33.550",
      "dateStarted": "2021-03-03 09:03:05.558",
      "dateFinished": "2021-03-03 09:03:05.732",
      "status": "FINISHED"
    },
    {
      "text": "%spark\nimport org.apache.spark.sql.functions._                       // Let us import the `desc` function\n\nval airlineCountByCountry \u003d airlineDS.groupBy(col(\"country\"))  // Create groups according to the \"Country\" column\n         .agg(count(\"country\") as \"airline-count\")            // Aggregate the size of the groups and count them\n         .orderBy(desc(\"airline-count\"))                      // Order them by largest\n\nairlineCountByCountry.show\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 09:04:47.420",
      "progress": 75,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614697203482_573241868",
      "id": "paragraph_1614697203482_573241868",
      "dateCreated": "2021-03-02 15:00:03.486",
      "dateStarted": "2021-03-03 09:04:40.155",
      "dateFinished": "2021-03-03 09:04:44.014",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Save it forever\nNow that we have our result, let\u0027s save it back to S3 so we can cherish the results forever.",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 08:46:18.503",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSave it forever\u003c/h2\u003e\n\u003cp\u003eNow that we have our result, let\u0026rsquo;s save it back to S3 so we can cherish the results forever.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614761140739_661165065",
      "id": "paragraph_1614761140739_661165065",
      "dateCreated": "2021-03-03 08:45:40.742",
      "dateStarted": "2021-03-03 08:46:18.502",
      "dateFinished": "2021-03-03 08:46:18.604",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\nairlineCountByCountry\n    .coalesce(1)                  // This ensures that we output 1 file only\n    .write                        // write the new results\n    .mode(\"overwrite\")            // Overwrite any \"old\" data\n    .parquet(\"s3a://word-count/airlines-count/airlines-by-country\") // location to save to\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 09:12:32.033",
      "progress": 99,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://9cd348169402:4040/jobs/job?id\u003d11"
            },
            {
              "jobUrl": "http://9cd348169402:4040/jobs/job?id\u003d12"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614761180110_354405934",
      "id": "paragraph_1614761180110_354405934",
      "dateCreated": "2021-03-03 08:46:20.110",
      "dateStarted": "2021-03-03 08:51:32.861",
      "dateFinished": "2021-03-03 08:51:37.117",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n## That\u0027s it!\n\nWe\u0027ve shown how to:\n1. Load data from s3\n2. Different ways of representing it (Dataframes vs. DataSets)\n3. Did a basic aggregation and count problem\n4. Saved the results back to MinIO\n5. Done so in a Zeppelin notebook enviroment with no local installations\n\n## Where do you go from here?\nZeppelin provides additional notebooks and interpreters to experimet with.\nI think it\u0027s a good way to get a \"taste\" of how to work with the different data engines and experiment.\nYou can try going throught the different example notebooks in Zeppelin.\n\nEnjoy!\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-03 09:22:42.872",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eThat\u0026rsquo;s it!\u003c/h2\u003e\n\u003cp\u003eWe\u0026rsquo;ve shown how to:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLoad data from s3\u003c/li\u003e\n\u003cli\u003eDifferent ways of representing it (Dataframes vs. DataSets)\u003c/li\u003e\n\u003cli\u003eDid a basic aggregation and count problem\u003c/li\u003e\n\u003cli\u003eSaved the results back to MinIO\u003c/li\u003e\n\u003cli\u003eDone so in a Zeppelin notebook enviroment with no local installations\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eWhere do you go from here?\u003c/h2\u003e\n\u003cp\u003eZeppelin provides additional notebooks and interpreters to experimet with.\u003cbr /\u003e\nI think it\u0026rsquo;s a good way to get a \u0026ldquo;taste\u0026rdquo; of how to work with the different data engines and experiment.\u003cbr /\u003e\nYou can try going throught the different example notebooks in Zeppelin.\u003c/p\u003e\n\u003cp\u003eEnjoy!\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614697220622_1719153455",
      "id": "paragraph_1614697220622_1719153455",
      "dateCreated": "2021-03-02 15:00:20.622",
      "dateStarted": "2021-03-03 09:22:42.852",
      "dateFinished": "2021-03-03 09:22:42.921",
      "status": "FINISHED"
    }
  ],
  "name": "Spark Airline Count Example",
  "id": "2F8KN6TKG",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}