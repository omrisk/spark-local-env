{
  "paragraphs": [
    {
      "title": "Use Generic Inline Configuration instead of Interpreter Setting",
      "text": "%md\n## Setting up the spark environment\nThis is the starting point of the wordcount example.\nWe will setup the basic configurations needed to setup our spark application and read information from our mock s3 bucket.\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-15 15:12:19.812",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSetting up the spark environment\u003c/h2\u003e\n\u003cp\u003eThis is the starting point of the wordcount example.\u003cbr /\u003e\nWe will setup the basic configurations needed to setup our spark application and read information from our mock s3 bucket.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588214762316_737450410",
      "id": "20180531-100923_1307061430",
      "dateCreated": "2020-04-30 10:46:02.000",
      "dateStarted": "2021-02-15 15:12:17.416",
      "dateFinished": "2021-02-15 15:12:17.437",
      "status": "FINISHED"
    },
    {
      "title": "Inline Configuration for standalone spark cluster",
      "text": "%spark.conf\n# This will point to the path we installed spark on the zeppelin docker image\nSPARK_HOME /opt/zeppelin/spark/\n\n# Will set the application name in the spark UI\nspark.app.name Zeppelin\n\n# set driver memory to 8g\nspark.driver.memory 8g\n\n# set executor memory 4g\nspark.executor.memory  4g\nspark.executor.cores 4\n\n# set execution mode\nmaster standalone\n\n# Will set the spark dirver to be our zeppelin docker\nspark.submit.deployMode\tclient\n\n# Needed for our writing efficiently to our s3 minio docker - read more at https://spark.apache.org/docs/3.0.1/cloud-integration.html\nspark.speculation false\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2\nspark.hadoop.fs.s3.impl org.apache.hadoop.fs.s3a.S3AFileSystem\n\n# Set to allow us to \"point\" to our mock s3 and use it interchangibly \nspark.hadoop.fs.s3a.endpoint http://minio:9000\nspark.hadoop.fs.s3a.path.style.access true\nspark.hadoop.fs.s3a.connection.ssl.enabled false\n\n# Any other spark properties can be set here. Here\u0027s avaliable spark configruation you can set. (http://spark.apache.org/docs/latest/configuration.html)",
      "user": "anonymous",
      "dateUpdated": "2021-02-19 07:14:38.183",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588214762316_1311021507",
      "id": "20180531-101615_648039641",
      "dateCreated": "2020-04-30 10:46:02.000",
      "dateStarted": "2021-02-19 07:11:06.321",
      "dateFinished": "2021-02-19 07:11:06.344",
      "status": "FINISHED"
    },
    {
      "text": "%md \n### Sanity test!\nYou can run the paragraph on the right to try a simple spark function that reads a local parquet file and \nprints it to the output section.\nThis will launch a spark application that will read a small `parquet` file and print it\u0027s schema and contents \nto the notebook.\nYou can visit the [spark UI](localhost:8080) or the [spark driver](localhost:4040) to check it\u0027s progress.",
      "user": "anonymous",
      "dateUpdated": "2021-02-18 12:18:05.958",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "tableHide": false,
        "results": {},
        "enabled": true,
        "editorSetting": {},
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eSanity test!\u003c/h3\u003e\n\u003cp\u003eYou can run the paragraph on the right to try a simple spark function that reads a local parquet file and\u003cbr /\u003e\nprints it to the output section.\u003cbr /\u003e\nThis will launch a spark application that will read a small \u003ccode\u003eparquet\u003c/code\u003e file and print it\u0026rsquo;s schema and contents\u003cbr /\u003e\nto the notebook.\u003cbr /\u003e\nYou can visit the \u003ca href\u003d\"localhost:8080\"\u003espark UI\u003c/a\u003e or the \u003ca href\u003d\"localhost:4040\"\u003espark driver\u003c/a\u003e to check it\u0026rsquo;s progress.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613400399813_1327092738",
      "id": "paragraph_1613400399813_1327092738",
      "dateCreated": "2021-02-15 14:46:39.813",
      "dateStarted": "2021-02-18 12:18:05.967",
      "dateFinished": "2021-02-18 12:18:08.055",
      "status": "FINISHED"
    },
    {
      "title": "",
      "text": "%spark\n\nimport spark.implicits._\n\nval df \u003d spark.read.format(\"parquet\").load(\"/zeppelin/example-files/users.parquet\")\ndf.printSchema\ndf.show\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-19 07:11:08.662",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://5f2cffb14a89:4040/jobs/job?id\u003d0"
            },
            {
              "jobUrl": "http://5f2cffb14a89:4040/jobs/job?id\u003d1"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588214762324_60233930",
      "id": "20180530-222838_1995256600",
      "dateCreated": "2020-04-30 10:46:02.000",
      "dateStarted": "2021-02-19 07:11:08.666",
      "dateFinished": "2021-02-19 07:11:34.178",
      "status": "FINISHED"
    },
    {
      "text": "%md \n## Reading from s3\nSince we\u0027ve setup a [minio](https://github.com/minio/minio) s3 docker, we can now read \nour airlines dataset into our spark application",
      "user": "anonymous",
      "dateUpdated": "2021-02-19 07:20:29.156",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "editorHide": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 90.4545,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eReading from s3\u003c/h2\u003e\n\u003cp\u003eSince we\u0026rsquo;ve setup a \u003ca href\u003d\"https://github.com/minio/minio\"\u003eminio\u003c/a\u003e s3 docker, we can now read\u003cbr /\u003e\nour airlines dataset into our spark application\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613402011974_1184306943",
      "id": "paragraph_1613402011974_1184306943",
      "dateCreated": "2021-02-15 15:13:31.974",
      "dateStarted": "2021-02-19 07:14:51.474",
      "dateFinished": "2021-02-19 07:14:53.428",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\nimport spark.implicits._\n\nval airlines \u003d spark.read.format(\"csv\").load(\"s3a://word-count/flights-data/airlines.csv\")\n\nairlines.printSchema\nairlines.show",
      "user": "anonymous",
      "dateUpdated": "2021-02-19 07:20:26.421",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613403849055_1657905061",
      "id": "paragraph_1613403849055_1657905061",
      "dateCreated": "2021-02-15 15:44:09.056",
      "status": "READY"
    },
    {
      "text": "%md\n## The word count exersize \nNow that you have a dataset loaded from \"s3\" you can experiment localy and attempt to solve the notorious \"word-count\" challenge and count how many different airlines run in each country.\n\nTry adding another `%spark` paragraph by clicking beneath this one and start experimenting!\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-19 07:20:32.507",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eThe word count exersize\u003c/h2\u003e\n\u003cp\u003eNow that you have a dataset loaded from \u0026ldquo;s3\u0026rdquo; you can experiment localy and attempt to solve the notorious \u0026ldquo;word-count\u0026rdquo; challenge and count how many different airlines run in each country.\u003c/p\u003e\n\u003cp\u003eTry adding another \u003ccode\u003e%spark\u003c/code\u003e paragraph by clicking beneath this one and start experimenting!\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613718993550_1857939506",
      "id": "paragraph_1613718993550_1857939506",
      "dateCreated": "2021-02-19 07:16:33.550",
      "dateStarted": "2021-02-19 07:20:31.219",
      "dateFinished": "2021-02-19 07:20:31.248",
      "status": "FINISHED"
    }
  ],
  "name": "Spark Word Count Example",
  "id": "2F8KN6TKG",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}